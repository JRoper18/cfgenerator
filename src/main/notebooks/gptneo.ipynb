{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting kernel\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting kernel\")\n",
    "param_size = \"125M\"\n",
    "pretrained_name = \"EleutherAI/gpt-neo-%s\" % param_size\n",
    "output_dir = \"./results-%s\" % param_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "model = GPTNeoForCausalLM.from_pretrained(pretrained_name).cuda()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_name, \n",
    "    bos_token=\"<|startoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|pad|>\")\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 595MB with 711/11554 MB allocated\n"
     ]
    }
   ],
   "source": [
    "mb = 10 ** 6\n",
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory / mb\n",
    "r = torch.cuda.memory_reserved(0) / mb\n",
    "a = torch.cuda.memory_allocated(0) / mb\n",
    "f = r-a  # free inside reserved\n",
    "print(\"Free memory: %dMB with %d/%d MB allocated\" % (f, a, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average/Max length to pad: 547/943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset_strs = []\n",
    "with open('../../../output/output10000.txt', 'r') as data:\n",
    "    split_ds = data.read().split(\"<|splitter|>\")\n",
    "    dataset_strs = [text for text in split_ds]\n",
    "token_lengths = [len(tokenizer.encode(ds_str)) for ds_str in dataset_strs]\n",
    "max_length = max(token_lengths)\n",
    "avg_length = sum(token_lengths) / len(token_lengths)\n",
    "print(\"Average/Max length to pad: %d/%d\" % (avg_length, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class ProgramDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            # Encode the descriptions using the GPT-Neo tokenizer\n",
    "            encodings_dict = tokenizer(\"<|startoftext|>\" \n",
    "                                        + txt +    \n",
    "                                        \"<|endoftext|>\",\n",
    "                                        truncation=True,\n",
    "                                        max_length=max_length, \n",
    "                                        padding=\"max_length\")\n",
    "            input_ids = torch.tensor(encodings_dict[\"input_ids\"])    \n",
    "            self.input_ids.append(input_ids)\n",
    "            mask = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "            self.attn_masks.append(mask)\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):    \n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Str dataset size: 10001\n",
      "Training size: 9000\n"
     ]
    }
   ],
   "source": [
    "# Now, tokenize the datasets.\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "print(\"Str dataset size: %d\" % len(dataset_strs))\n",
    "dataset = ProgramDataset(dataset_strs, tokenizer, max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_ds, eval_ds = random_split(dataset, [train_size, len(dataset) - train_size], generator=torch.Generator().manual_seed(42))\n",
    "print(\"Training size: %d\" % train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture stored_output\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "# with torch.no_grad():\n",
    "#     print(torch.cuda.is_available()) \n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,         # output directory\n",
    "    num_train_epochs=4,              # total # of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    # no_cuda = True, # Damn these GPUs really are small\n",
    "    fp16=True # FOr better memory and training speeds. \n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it\n",
    "    data_collator=lambda data: \n",
    "              {\"input_ids\": torch.stack([f[0] for f in data]),       \n",
    "               \"attention_mask\": torch.stack([f[1] for f in data]),\n",
    "               \"labels\": torch.stack([f[0] for f in data])}\n",
    ")\n",
    "# train_result = trainer.train()\n",
    "train_result = trainer.train(resume_from_checkpoint=True)\n",
    "print(\"done training!\")\n",
    "trainer.save_model(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, make the outputs for us to evaluate:\n",
    "fine_model = GPTNeoForCausalLM.from_pretrained(output_dir).cuda()\n",
    "fine_tokenizer = GPT2Tokenizer.from_pretrained(output_dir, \n",
    "    bos_token=\"<|startoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|pad|>\"\n",
    ")\n",
    "\n",
    "# input_tensor = fine_tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids.cuda()\n",
    "eval_loader = torch.utils.data.DataLoader(eval_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "with open('../../../output/gpt-eval.txt', 'a') as file:\n",
    "    for eval_ex in eval_ds:\n",
    "        eval_input_ids = eval_ex[0]\n",
    "        chopped_str = fine_tokenizer.decode(eval_input_ids).split(\"Program:\")\n",
    "        input_str = chopped_str[0] + \"\\nProgram:\\n\"\n",
    "        input_tensor = fine_tokenizer.encode(input_str, return_tensors=\"pt\").cuda()\n",
    "        outputs = fine_model.generate(\n",
    "            input_tensor, \n",
    "            max_length=1200,  \n",
    "            # num_return_sequences=5,\n",
    "            # no_repeat_ngram_size=2,\n",
    "            # repetition_penalty=1.5,\n",
    "            top_p=0.95,\n",
    "            temperature=.55,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            # early_stopping=False\n",
    "        )\n",
    "        total_output = tokenizer.decode(outputs[0])\n",
    "        total_output = total_output.replace(\"<|startoftext|>\", \"<|splitter|>\")\n",
    "        total_output = total_output.replace(\"<|endoftext|>\", \"\")\n",
    "        file.write(total_output)\n",
    "\n",
    "print(\"Done writing evaluation to file!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2282e338ecec3ece71c2002cd0caa8a2ea177485a557703c3a2b1c6e231f9179"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('gpt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
